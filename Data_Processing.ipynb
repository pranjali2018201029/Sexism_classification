{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read csv into pandas DataFrame and drop memtioned columns\n",
    "\n",
    "def Read_DataFile(filepath, Columns_to_drop=[]):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df = df.drop(columns=Columns_to_drop)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge multiple datafiles into single file (all files must have same columns)\n",
    "\n",
    "def Merge_DataFiles(filepath_list, Output_filepath):\n",
    "    \n",
    "    merged_df = Read_DataFile(filepath_list[0])\n",
    "    for i in range(1,len(filepath_list)):\n",
    "        filepath = filepath_list[i]\n",
    "        temp_df = Read_DataFile(filepath)\n",
    "        merged_df = pd.concat([merged_df, temp_df])\n",
    "    \n",
    "    merged_df.to_csv(Output_filepath)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preprocessing on caption, comments, hashtags\n",
    "\n",
    "def Data_Processing(data):\n",
    "    \n",
    "    Column_list = [\"Hashtags\", \"Tags_Len\", \"Caption_Tokens\", \"Cap_Tokens_Len\", \"Comments_Tokens\", \"Com_Tokens_Len\"]\n",
    "    processed_df = pd.DataFrame(columns=Column_list)\n",
    "    Req_Columns = [\"text\",\"comments\",\"hashtags\"]\n",
    "    Req_Data = data[Req_Columns]\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z0-9_#]+')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    for row in range(Req_Data.shape[0]):\n",
    "        \n",
    "        Hashtags = set()\n",
    "        Caption = []\n",
    "        Comments = []\n",
    "        \n",
    "        for col in Req_Columns:\n",
    "            \n",
    "            text_content = Req_Data.iloc[row][col]\n",
    "            \n",
    "            if type(text_content) != str:\n",
    "                continue\n",
    "\n",
    "            elif col==\"hashtags\":\n",
    "\n",
    "                ## Tokenize with delimiter space, No further processing for hashtags\n",
    "                tokens = text_content.split()\n",
    "                \n",
    "                for token in tokens:\n",
    "                    if token.startswith('#'):\n",
    "                        Hashtags.add(token)\n",
    "            else:\n",
    "\n",
    "                ## Tokenize with delimiter space\n",
    "                tokens = tokenizer.tokenize(text_content)\n",
    "\n",
    "                for token in tokens:\n",
    "\n",
    "                    ## Remove numbers\n",
    "                    if token.isnumeric():\n",
    "                        continue\n",
    "                    \n",
    "                    ## Remove hashtags from caption, comments and add them to hashtags list\n",
    "                    elif token.startswith('#'):\n",
    "                        Hashtags.add(token)\n",
    "                    \n",
    "                    ## Remove stopwords\n",
    "                    elif token.casefold() in stop_words:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if col == \"text\":\n",
    "                            Caption.append(token.casefold())\n",
    "                        else:\n",
    "                            Comments.append(token.casefold()) \n",
    "        \n",
    "        Hashtag_string = ' '.join(str(tag) for tag in Hashtags)\n",
    "        Caption_string = ' '.join(str(c) for c in Caption)\n",
    "        Comments_string = ' '.join(str(c) for c in Comments)\n",
    "        \n",
    "        temp_df = pd.DataFrame([[Hashtag_string,len(Hashtags),Caption_string,len(Caption),Comments_string,len(Comments)]], columns=Column_list)\n",
    "       \n",
    "        if row==0:\n",
    "            processed_df = temp_df\n",
    "        else:\n",
    "            processed_df = pd.concat([processed_df, temp_df], ignore_index=True)\n",
    "        \n",
    "    return processed_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Filter_Data(Data, Hashtags_Threshold=1, Caption_Threshold=10, Comments_Threshold=10):\n",
    "    \n",
    "    print(\"Original data shape: \", Data.shape)\n",
    "    index_to_drop = []\n",
    "    \n",
    "    for row in range(Data.shape[0]):\n",
    "        \n",
    "        row_entry = Data.iloc[row] \n",
    "        flag = 0\n",
    "        \n",
    "        if row_entry['Tags_Len'].item() < Hashtags_Threshold:\n",
    "            flag = 1\n",
    "        elif row_entry['Cap_Tokens_Len'].item() < Caption_Threshold:\n",
    "            flag = 1\n",
    "        elif row_entry['Com_Tokens_Len'].item() < Comments_Threshold:\n",
    "            flag = 1\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        if flag==1:\n",
    "            index_to_drop.append(Data.index[row])\n",
    "            \n",
    "    Filtered_Data = Data.drop(index=index_to_drop)      \n",
    "    print(\"Filtered data shape: \", Filtered_Data.shape)\n",
    "    \n",
    "    return Filtered_Data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:9: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Data Shape:  (12484, 6)\n",
      "Processed Data Shape:  (12484, 6)\n"
     ]
    }
   ],
   "source": [
    "Files_to_merge = [\"./Data/everydaysexism.csv\",\"./Data/genderbias.csv\",\"./Data/genderstereotype.csv\",\"./Data/heforshe.csv\",\n",
    "                  \"./Data/mencallmethings.csv\",\"./Data/metoo.csv\",\"./Data/misogynist.csv\",\"./Data/notallmen.csv\",\n",
    "                  \"./Data/questionsformen.csv\",\"./Data/slutgate.csv\",\"./Data/wagegap.csv\",\"./Data/weareequal.csv\",\n",
    "                  \"./Data/womenareinferior.csv\",\"./Data/workplaceharassment.csv\",\"./Data/yesallwomen.csv\"]\n",
    "\n",
    "merged_df = Merge_DataFiles(Files_to_merge, \"./Data/Merged_Data.csv\")\n",
    "print(\"Merged Data Shape: \",merged_df.shape)\n",
    "\n",
    "Processed_Df = Data_Processing(merged_df)\n",
    "print(\"Processed Data Shape: \", Processed_Df.shape)\n",
    "Processed_Df.to_csv(\"./Data/Processed_Data.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filtered_Data = Filter_Data(Processed_Df,1,10,10)\n",
    "print(Filtered_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge all files of different hashtags into one file, Drop unnecessary columns\n",
    "## Remove non_english text(caption, hashtags, comments) from the post\n",
    "## Tokenize text of each post\n",
    "## Remove punctuations(except delimiters used), emojis, numbers, stopwords from text(caption, comments)\n",
    "## Data Analysis: length_of_text vs no_of_posts (Zipf distribution), no_of_hashtags vs no_of_posts,    \n",
    "##                Distribution of posts as per hashtags\n",
    "## Remove posts having text of length less than particular threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
