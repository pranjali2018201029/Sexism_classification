{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iASScSNJ3JyR",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "juvZnk4i3Vdg",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Flatten"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xN3FNfks57jx",
        "colab": {}
      },
      "source": [
        "def Create_LSTM_Model(No_Features=300, No_Hidden_Layers=2, No_Hidden_Neurons=150, \n",
        "                    Hidden_Activation =\"relu\", No_OP_Neurons=1, \n",
        "                    Output_Activation=\"sigmoid\", Kernel_Initializer=\"random_normal\",\n",
        "                    Optimizer=\"adam\", Loss='binary_crossentropy', Metrics =['accuracy']):\n",
        "  \n",
        "  classifier = Sequential()\n",
        "\n",
        "  ## Input Layer\n",
        "  # classifier.add(Dense(No_Hidden_Neurons, activation=Hidden_Activation, \n",
        "  #                      kernel_initializer=Kernel_Initializer, input_dim=No_Features))\n",
        "  \n",
        "  classifier.add(LSTM(No_Hidden_Neurons, input_shape=(1,No_Features), return_sequences=True,\n",
        "                      activation=Hidden_Activation, kernel_initializer=Kernel_Initializer))\n",
        "  \n",
        "  ## Hidden layers with default tanh activation and sigmoid recurrent_activation\n",
        "  for i in range(No_Hidden_Layers):\n",
        "    classifier.add(LSTM(No_Hidden_Neurons, return_sequences=True))\n",
        "  classifier.add(Flatten())\n",
        "  ## Output Layer\n",
        "  classifier.add(Dense(No_OP_Neurons, activation=Output_Activation, \n",
        "                       kernel_initializer=Kernel_Initializer))\n",
        "  \n",
        "  classifier.compile(optimizer =Optimizer, loss=Loss, metrics = Metrics)\n",
        "\n",
        "  return classifier\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2WZnhN8wN-fV",
        "colab": {}
      },
      "source": [
        "def Train_LSTM(LSTM_classifier, train_data, feature_list=[], Batch_Size=50, Epochs=100):\n",
        "\n",
        "  train_data.dropna()\n",
        "  train_data = pd.DataFrame(np.nan_to_num(np.array(train_data)), columns = train_data.columns)\n",
        "  train_data['Label'] = pd.to_numeric(train_data['Label'], errors='coerce')\n",
        "  train_data = train_data.dropna(subset=['Label'])\n",
        "  \n",
        "  train_features = train_data[feature_list]    \n",
        "  train_labels = train_data[\"Label\"]\n",
        "  train_labels = train_labels.astype('int')\n",
        "\n",
        "  print(\"train_features shape: \", train_features.shape)\n",
        "  # print(\"train_features col: \", train_features.columns)\n",
        "  # print(\"train_features head: \", train_features.head)\n",
        "  train_features = np.array(train_features)\n",
        "  train_features = np.reshape(train_features, (train_features.shape[0], 1, train_features.shape[1]))\n",
        "  print(\"train_features shape: \", train_features.shape)\n",
        "  print(train_labels.shape)\n",
        "  LSTM_classifier.fit(train_features,train_labels, batch_size=Batch_Size, epochs=Epochs)\n",
        "  eval_model=LSTM_classifier.evaluate(train_features, train_labels)\n",
        "\n",
        "  print(\"Loss: \", eval_model[0])\n",
        "  print(\"Accuracy of the model: \", eval_model[1])\n",
        "  return LSTM_classifier\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jZKBao4vOIsw",
        "colab": {}
      },
      "source": [
        "## Store trained model in a file to reuse in other codes without training again on same data\n",
        "\n",
        "def Store_Trained_LSTM(LSTM_obj, Filepath):\n",
        "  \n",
        "  with open(Filepath, \"wb\") as file:\n",
        "    pickle.dump(LSTM_obj, file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ijXfOZntONnS",
        "colab": {}
      },
      "source": [
        "## Load stored trained model and returns random forest model object\n",
        "\n",
        "def Load_Trained_LSTM(Filepath):\n",
        "  \n",
        "  with open(Filepath, \"rb\") as file:\n",
        "    LSTM_obj = pickle.load(file)\n",
        "\n",
        "  return LSTM_obj"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fm_aS3D9OR7x",
        "colab": {}
      },
      "source": [
        "def Evaluate_LSTM(test_data, LSTM_Model_FilePath, feature_list=[], threshold=0.5):\n",
        "  \n",
        "  test_data.dropna()\n",
        "  test_data = pd.DataFrame(np.nan_to_num(np.array(test_data)),  columns = test_data.columns)\n",
        "  test_data['Label'] = pd.to_numeric(test_data['Label'], errors='coerce')\n",
        "  test_data = test_data.dropna(subset=['Label'])\n",
        "\n",
        "  test_features = test_data[feature_list]\n",
        "  test_labels = test_data[\"Label\"]\n",
        "  test_labels = test_labels.astype('int')\n",
        "  # print(test_features)\n",
        "  test_features=np.array(test_features);\n",
        "  test_features = np.reshape(test_features, (test_features.shape[0], 1, test_features.shape[1]))\n",
        "  LSTM_obj = Load_Trained_LSTM(LSTM_Model_FilePath) \n",
        "  # print(test_features.shape)\n",
        "  predictions = LSTM_obj.predict(test_features)\n",
        "  predictions_list = [int(p[0]) for p in predictions]\n",
        "  \n",
        "  for i in range(len(predictions_list)):\n",
        "    if predictions_list[i] >= threshold:\n",
        "      predictions_list[i] = 1\n",
        "    else:\n",
        "      predictions_list[i] = 0\n",
        "  \n",
        "  errors = abs(predictions_list - test_labels)\n",
        "\n",
        "  # Calculate mean absolute error (MAE)\n",
        "  MAE = round(np.mean(errors), 2)\n",
        "  \n",
        "  ## Confusion Matrix and Classification Report\n",
        "  Confusion_Matrix = confusion_matrix(test_labels,predictions_list)\n",
        "  Report = classification_report(test_labels,predictions_list)\n",
        "  \n",
        "  return MAE, Confusion_Matrix, Report\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hKfRggmLQUka",
        "colab": {}
      },
      "source": [
        "## WORD2VEC EMBEDDINGS\n",
        "\n",
        "Column_List = [ \"Caption\"]\n",
        "Vector_Size = 300\n",
        "Embedding_Cols = [str(i) for i in range(Vector_Size)]\n",
        "Column_List.extend(Embedding_Cols)\n",
        "Column_List.append(\"Label\")\n",
        "\n",
        "Train_Embedding_FilePath = \"/content/TrainData_Word2Vec_Embeddings.csv\"\n",
        "Test_Embedding_FilePath = \"/content/TestData_Word2Vec_Embeddings.csv\"\n",
        "LSTM_Model_FilePath = \"/content/LSTM_Word2Vec_Train_Model.pkl\"\n",
        "\n",
        "train_data = pd.read_csv(Train_Embedding_FilePath, usecols=Column_List)\n",
        "test_data = pd.read_csv(Test_Embedding_FilePath, usecols=Column_List)\n",
        "print(train_data.shape)\n",
        "print(len(Embedding_Cols))\n",
        "## Training Phase\n",
        "LSTM_Classifier = Create_LSTM_Model()\n",
        "LSTM_obj = Train_LSTM(LSTM_Classifier, train_data, Embedding_Cols)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qunWXfag83vn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Store_Trained_LSTM(LSTM_obj, LSTM_Model_FilePath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cMUayndftrRK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "7bf46c3b-7a9e-422a-f467-1bfd57a02123"
      },
      "source": [
        "## Testing Phase\n",
        "MAE, Confusion_Matrix, Report = Evaluate_LSTM(test_data, LSTM_Model_FilePath, Embedding_Cols, 0.5)\n",
        "\n",
        "print(\"============ FOR WORD2VEC EMBEDDINGS ============\")\n",
        "\n",
        "print(\"MEAN ABSOLUTE ERROR: \", MAE)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"============ CONFUSION MATRIX ===============\")\n",
        "print(Confusion_Matrix)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"============ CLASSIFICATION REPORT ==============\")\n",
        "print(Report)\n",
        "\n",
        "tn, fp, fn, tp = Confusion_Matrix.ravel()\n",
        "Accuracy = (tn+tp)/(tn + fp + fn + tp)\n",
        "\n",
        "print(\"Accuracy: \", Accuracy*100)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============ FOR WORD2VEC EMBEDDINGS ============\n",
            "MEAN ABSOLUTE ERROR:  0.22\n",
            "\n",
            "\n",
            "============ CONFUSION MATRIX ===============\n",
            "[[719  14]\n",
            " [310 413]]\n",
            "\n",
            "\n",
            "============ CLASSIFICATION REPORT ==============\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.98      0.82       733\n",
            "           1       0.97      0.57      0.72       723\n",
            "\n",
            "    accuracy                           0.78      1456\n",
            "   macro avg       0.83      0.78      0.77      1456\n",
            "weighted avg       0.83      0.78      0.77      1456\n",
            "\n",
            "Accuracy:  77.74725274725274\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qK25x_1_s9A4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3bfbd716-b436-4e8e-bbef-722a504a0588"
      },
      "source": [
        "## GLOVE EMBEDDINGS\n",
        "\n",
        "Column_List = [ \"Caption_Tokens\"]\n",
        "Vector_Size = 300\n",
        "Embedding_Cols = [str(i) for i in range(Vector_Size)]\n",
        "Column_List.extend(Embedding_Cols)\n",
        "Column_List.append(\"Label\")\n",
        "\n",
        "Train_Embedding_FilePath = \"/content/TrainData_Glove_Embeddings.csv\"\n",
        "Test_Embedding_FilePath = \"/content/TestData_Glove_Embeddings.csv\"\n",
        "LSTM_Model_FilePath = \"/content/LSTM_Glove_Train_Model.pkl\"\n",
        "\n",
        "train_data = pd.read_csv(Train_Embedding_FilePath, usecols=Column_List)\n",
        "test_data = pd.read_csv(Test_Embedding_FilePath, usecols=Column_List)\n",
        "\n",
        "## Training Phase\n",
        "LSTM_Classifier = Create_LSTM_Model()\n",
        "LSTM_obj = Train_LSTM(LSTM_Classifier, train_data, Embedding_Cols)\n",
        "Store_Trained_LSTM(LSTM_obj, LSTM_Model_FilePath)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_features shape:  (5824, 300)\n",
            "train_features shape:  (5824, 1, 300)\n",
            "(5824,)\n",
            "Epoch 1/100\n",
            "5824/5824 [==============================] - 16s 3ms/step - loss: 0.5896 - acc: 0.6604\n",
            "Epoch 2/100\n",
            "5824/5824 [==============================] - 4s 661us/step - loss: 0.4895 - acc: 0.7517\n",
            "Epoch 3/100\n",
            "5824/5824 [==============================] - 4s 645us/step - loss: 0.4705 - acc: 0.7634\n",
            "Epoch 4/100\n",
            "5824/5824 [==============================] - 4s 633us/step - loss: 0.4540 - acc: 0.7709\n",
            "Epoch 5/100\n",
            "5824/5824 [==============================] - 4s 643us/step - loss: 0.4489 - acc: 0.7737\n",
            "Epoch 6/100\n",
            "5824/5824 [==============================] - 4s 633us/step - loss: 0.4378 - acc: 0.7802\n",
            "Epoch 7/100\n",
            "5824/5824 [==============================] - 4s 630us/step - loss: 0.4308 - acc: 0.7826\n",
            "Epoch 8/100\n",
            "5824/5824 [==============================] - 4s 634us/step - loss: 0.4265 - acc: 0.7867\n",
            "Epoch 9/100\n",
            "5824/5824 [==============================] - 4s 640us/step - loss: 0.4295 - acc: 0.7855\n",
            "Epoch 10/100\n",
            "5824/5824 [==============================] - 4s 634us/step - loss: 0.4196 - acc: 0.7909\n",
            "Epoch 11/100\n",
            "5824/5824 [==============================] - 4s 641us/step - loss: 0.4139 - acc: 0.7953\n",
            "Epoch 12/100\n",
            "5824/5824 [==============================] - 4s 633us/step - loss: 0.4125 - acc: 0.7922\n",
            "Epoch 13/100\n",
            "5824/5824 [==============================] - 4s 637us/step - loss: 0.4105 - acc: 0.7976\n",
            "Epoch 14/100\n",
            "5824/5824 [==============================] - 4s 640us/step - loss: 0.4073 - acc: 0.7984\n",
            "Epoch 15/100\n",
            "5824/5824 [==============================] - 4s 628us/step - loss: 0.4013 - acc: 0.7998\n",
            "Epoch 16/100\n",
            "5824/5824 [==============================] - 4s 628us/step - loss: 0.4026 - acc: 0.8055\n",
            "Epoch 17/100\n",
            "5824/5824 [==============================] - 4s 622us/step - loss: 0.3991 - acc: 0.8056\n",
            "Epoch 18/100\n",
            "5824/5824 [==============================] - 4s 630us/step - loss: 0.3976 - acc: 0.8070\n",
            "Epoch 19/100\n",
            "5824/5824 [==============================] - 4s 632us/step - loss: 0.3962 - acc: 0.8079\n",
            "Epoch 20/100\n",
            "5824/5824 [==============================] - 4s 635us/step - loss: 0.3880 - acc: 0.8073\n",
            "Epoch 21/100\n",
            "5824/5824 [==============================] - 4s 626us/step - loss: 0.3891 - acc: 0.8056\n",
            "Epoch 22/100\n",
            "5824/5824 [==============================] - 4s 624us/step - loss: 0.3861 - acc: 0.8139\n",
            "Epoch 23/100\n",
            "5824/5824 [==============================] - 4s 625us/step - loss: 0.3826 - acc: 0.8149\n",
            "Epoch 24/100\n",
            "5824/5824 [==============================] - 4s 622us/step - loss: 0.3767 - acc: 0.8151\n",
            "Epoch 25/100\n",
            "5824/5824 [==============================] - 4s 629us/step - loss: 0.3733 - acc: 0.8156\n",
            "Epoch 26/100\n",
            "5824/5824 [==============================] - 4s 613us/step - loss: 0.3732 - acc: 0.8175\n",
            "Epoch 27/100\n",
            "5824/5824 [==============================] - 4s 605us/step - loss: 0.3675 - acc: 0.8225\n",
            "Epoch 28/100\n",
            "5824/5824 [==============================] - 4s 628us/step - loss: 0.3679 - acc: 0.8218\n",
            "Epoch 29/100\n",
            "5824/5824 [==============================] - 4s 614us/step - loss: 0.3677 - acc: 0.8178\n",
            "Epoch 30/100\n",
            "5824/5824 [==============================] - 4s 606us/step - loss: 0.3670 - acc: 0.8189\n",
            "Epoch 31/100\n",
            "5824/5824 [==============================] - 4s 619us/step - loss: 0.3643 - acc: 0.8233\n",
            "Epoch 32/100\n",
            "5824/5824 [==============================] - 4s 642us/step - loss: 0.3586 - acc: 0.8254\n",
            "Epoch 33/100\n",
            "5824/5824 [==============================] - 4s 638us/step - loss: 0.3572 - acc: 0.8221\n",
            "Epoch 34/100\n",
            "5824/5824 [==============================] - 4s 614us/step - loss: 0.3538 - acc: 0.8259\n",
            "Epoch 35/100\n",
            "5824/5824 [==============================] - 4s 608us/step - loss: 0.3532 - acc: 0.8283\n",
            "Epoch 36/100\n",
            "5824/5824 [==============================] - 4s 620us/step - loss: 0.3486 - acc: 0.8329\n",
            "Epoch 37/100\n",
            "5824/5824 [==============================] - 4s 618us/step - loss: 0.3453 - acc: 0.8317\n",
            "Epoch 38/100\n",
            "5824/5824 [==============================] - 4s 612us/step - loss: 0.3502 - acc: 0.8283\n",
            "Epoch 39/100\n",
            "5824/5824 [==============================] - 4s 636us/step - loss: 0.3464 - acc: 0.8350\n",
            "Epoch 40/100\n",
            "5824/5824 [==============================] - 4s 635us/step - loss: 0.3433 - acc: 0.8333\n",
            "Epoch 41/100\n",
            "5824/5824 [==============================] - 4s 632us/step - loss: 0.3405 - acc: 0.8348\n",
            "Epoch 42/100\n",
            "5824/5824 [==============================] - 4s 628us/step - loss: 0.3418 - acc: 0.8359\n",
            "Epoch 43/100\n",
            "5824/5824 [==============================] - 4s 625us/step - loss: 0.3368 - acc: 0.8374\n",
            "Epoch 44/100\n",
            "5824/5824 [==============================] - 4s 633us/step - loss: 0.3350 - acc: 0.8360\n",
            "Epoch 45/100\n",
            "5824/5824 [==============================] - 4s 672us/step - loss: 0.3366 - acc: 0.8383\n",
            "Epoch 46/100\n",
            "5824/5824 [==============================] - 4s 647us/step - loss: 0.3312 - acc: 0.8391\n",
            "Epoch 47/100\n",
            "5824/5824 [==============================] - 4s 646us/step - loss: 0.3269 - acc: 0.8427\n",
            "Epoch 48/100\n",
            "5824/5824 [==============================] - 4s 643us/step - loss: 0.3303 - acc: 0.8383\n",
            "Epoch 49/100\n",
            "5824/5824 [==============================] - 4s 643us/step - loss: 0.3253 - acc: 0.8439\n",
            "Epoch 50/100\n",
            "5824/5824 [==============================] - 4s 639us/step - loss: 0.3222 - acc: 0.8439\n",
            "Epoch 51/100\n",
            "5824/5824 [==============================] - 4s 625us/step - loss: 0.3265 - acc: 0.8410\n",
            "Epoch 52/100\n",
            "5824/5824 [==============================] - 4s 633us/step - loss: 0.3209 - acc: 0.8448\n",
            "Epoch 53/100\n",
            "5824/5824 [==============================] - 4s 640us/step - loss: 0.3196 - acc: 0.8420\n",
            "Epoch 54/100\n",
            "5824/5824 [==============================] - 4s 631us/step - loss: 0.3207 - acc: 0.8437\n",
            "Epoch 55/100\n",
            "5824/5824 [==============================] - 4s 618us/step - loss: 0.3312 - acc: 0.8441\n",
            "Epoch 56/100\n",
            "5824/5824 [==============================] - 4s 636us/step - loss: 0.3314 - acc: 0.8360\n",
            "Epoch 57/100\n",
            "5824/5824 [==============================] - 4s 630us/step - loss: 0.3190 - acc: 0.8470\n",
            "Epoch 58/100\n",
            "5824/5824 [==============================] - 4s 643us/step - loss: 0.3146 - acc: 0.8489\n",
            "Epoch 59/100\n",
            "5824/5824 [==============================] - 4s 639us/step - loss: 0.3041 - acc: 0.8556\n",
            "Epoch 60/100\n",
            "5824/5824 [==============================] - 4s 641us/step - loss: 0.3040 - acc: 0.8582\n",
            "Epoch 61/100\n",
            "5824/5824 [==============================] - 4s 647us/step - loss: 0.2961 - acc: 0.8597\n",
            "Epoch 62/100\n",
            "5824/5824 [==============================] - 4s 651us/step - loss: 0.2967 - acc: 0.8587\n",
            "Epoch 63/100\n",
            "5824/5824 [==============================] - 4s 655us/step - loss: 0.2944 - acc: 0.8613\n",
            "Epoch 64/100\n",
            "5824/5824 [==============================] - 4s 635us/step - loss: 0.2958 - acc: 0.8628\n",
            "Epoch 65/100\n",
            "5824/5824 [==============================] - 4s 624us/step - loss: 0.2962 - acc: 0.8556\n",
            "Epoch 66/100\n",
            "5824/5824 [==============================] - 4s 634us/step - loss: 0.2918 - acc: 0.8614\n",
            "Epoch 67/100\n",
            "5824/5824 [==============================] - 4s 639us/step - loss: 0.2910 - acc: 0.8601\n",
            "Epoch 68/100\n",
            "5824/5824 [==============================] - 4s 625us/step - loss: 0.3064 - acc: 0.8551\n",
            "Epoch 69/100\n",
            "5824/5824 [==============================] - 4s 620us/step - loss: 0.2828 - acc: 0.8668\n",
            "Epoch 70/100\n",
            "5824/5824 [==============================] - 4s 628us/step - loss: 0.2767 - acc: 0.8673\n",
            "Epoch 71/100\n",
            "5824/5824 [==============================] - 4s 620us/step - loss: 0.2789 - acc: 0.8690\n",
            "Epoch 72/100\n",
            "5824/5824 [==============================] - 4s 633us/step - loss: 0.2802 - acc: 0.8650\n",
            "Epoch 73/100\n",
            "5824/5824 [==============================] - 4s 646us/step - loss: 0.2725 - acc: 0.8662\n",
            "Epoch 74/100\n",
            "5824/5824 [==============================] - 4s 638us/step - loss: 0.2953 - acc: 0.8680\n",
            "Epoch 75/100\n",
            "5824/5824 [==============================] - 4s 641us/step - loss: 0.2847 - acc: 0.8656\n",
            "Epoch 76/100\n",
            "5824/5824 [==============================] - 4s 642us/step - loss: 0.2688 - acc: 0.8740\n",
            "Epoch 77/100\n",
            "5824/5824 [==============================] - 4s 649us/step - loss: 0.2656 - acc: 0.8769\n",
            "Epoch 78/100\n",
            "5824/5824 [==============================] - 4s 639us/step - loss: 0.2611 - acc: 0.8819\n",
            "Epoch 79/100\n",
            "5824/5824 [==============================] - 4s 641us/step - loss: 0.2869 - acc: 0.8683\n",
            "Epoch 80/100\n",
            "5824/5824 [==============================] - 4s 631us/step - loss: 0.2605 - acc: 0.8791\n",
            "Epoch 81/100\n",
            "5824/5824 [==============================] - 4s 642us/step - loss: 0.2608 - acc: 0.8798\n",
            "Epoch 82/100\n",
            "5824/5824 [==============================] - 4s 630us/step - loss: 0.2641 - acc: 0.8796\n",
            "Epoch 83/100\n",
            "5824/5824 [==============================] - 4s 632us/step - loss: 0.2542 - acc: 0.8815\n",
            "Epoch 84/100\n",
            "5824/5824 [==============================] - 4s 618us/step - loss: 0.2499 - acc: 0.8829\n",
            "Epoch 85/100\n",
            "5824/5824 [==============================] - 4s 632us/step - loss: 0.2658 - acc: 0.8769\n",
            "Epoch 86/100\n",
            "5824/5824 [==============================] - 4s 617us/step - loss: 0.2506 - acc: 0.8865\n",
            "Epoch 87/100\n",
            "5824/5824 [==============================] - 4s 650us/step - loss: 0.2457 - acc: 0.8855\n",
            "Epoch 88/100\n",
            "5824/5824 [==============================] - 4s 628us/step - loss: 0.2363 - acc: 0.8899\n",
            "Epoch 89/100\n",
            "5824/5824 [==============================] - 4s 612us/step - loss: 0.2455 - acc: 0.8879\n",
            "Epoch 90/100\n",
            "5824/5824 [==============================] - 4s 628us/step - loss: 0.2421 - acc: 0.8875\n",
            "Epoch 91/100\n",
            "5824/5824 [==============================] - 4s 630us/step - loss: 0.2483 - acc: 0.8853\n",
            "Epoch 92/100\n",
            "5824/5824 [==============================] - 4s 634us/step - loss: 0.2320 - acc: 0.8930\n",
            "Epoch 93/100\n",
            "5824/5824 [==============================] - 4s 630us/step - loss: 0.2390 - acc: 0.8865\n",
            "Epoch 94/100\n",
            "5824/5824 [==============================] - 4s 625us/step - loss: 0.2357 - acc: 0.8894\n",
            "Epoch 95/100\n",
            "5824/5824 [==============================] - 4s 613us/step - loss: 0.2267 - acc: 0.8963\n",
            "Epoch 96/100\n",
            "5824/5824 [==============================] - 4s 644us/step - loss: 0.2241 - acc: 0.8975\n",
            "Epoch 97/100\n",
            "5824/5824 [==============================] - 4s 632us/step - loss: 0.2266 - acc: 0.8971\n",
            "Epoch 98/100\n",
            "5824/5824 [==============================] - 4s 620us/step - loss: 0.2292 - acc: 0.8949\n",
            "Epoch 99/100\n",
            "5824/5824 [==============================] - 4s 629us/step - loss: 0.2242 - acc: 0.8970\n",
            "Epoch 100/100\n",
            "5824/5824 [==============================] - 4s 617us/step - loss: 0.2320 - acc: 0.8937\n",
            "5824/5824 [==============================] - 5s 867us/step\n",
            "Loss:  0.1980643028004484\n",
            "Accuracy of the model:  0.9156936813186813\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3dOPBKHTtKLM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "44bb3da2-2ea8-411e-d8bc-1a5099bcc00f"
      },
      "source": [
        "## Testing Phase\n",
        "MAE, Confusion_Matrix, Report = Evaluate_LSTM(test_data, LSTM_Model_FilePath, Embedding_Cols, 0.5)\n",
        "\n",
        "print(\"============ FOR GLOVE EMBEDDINGS ============\")\n",
        "\n",
        "print(\"MEAN ABSOLUTE ERROR: \", MAE)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"============ CONFUSION MATRIX ===============\")\n",
        "print(Confusion_Matrix)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"============ CLASSIFICATION REPORT ==============\")\n",
        "print(Report)\n",
        "\n",
        "tn, fp, fn, tp = Confusion_Matrix.ravel()\n",
        "Accuracy = (tn+tp)/(tn + fp + fn + tp)\n",
        "\n",
        "print(\"Accuracy: \", Accuracy*100)"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============ FOR GLOVE EMBEDDINGS ============\n",
            "MEAN ABSOLUTE ERROR:  0.42\n",
            "\n",
            "\n",
            "============ CONFUSION MATRIX ===============\n",
            "[[714   3]\n",
            " [611 128]]\n",
            "\n",
            "\n",
            "============ CLASSIFICATION REPORT ==============\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      1.00      0.70       717\n",
            "           1       0.98      0.17      0.29       739\n",
            "\n",
            "    accuracy                           0.58      1456\n",
            "   macro avg       0.76      0.58      0.50      1456\n",
            "weighted avg       0.76      0.58      0.49      1456\n",
            "\n",
            "Accuracy:  57.829670329670336\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nfGOltXCQA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}