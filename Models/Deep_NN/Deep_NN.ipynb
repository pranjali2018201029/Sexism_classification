{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_NN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTJH0lVkqQQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSljQfgguOp8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "64247beb-934c-4e5a-e466-e3e43b37702a"
      },
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9XQBH7DzGH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Create_NN_Model(No_Features=300, No_Hidden_Layers=2, No_Hidden_Neurons=150, \n",
        "                    Hidden_Activation =\"relu\", No_OP_Neurons=1, \n",
        "                    Output_Activation=\"sigmoid\", Kernel_Initializer=\"random_normal\",\n",
        "                    Optimizer=\"adam\", Loss='binary_crossentropy', Metrics =['accuracy']):\n",
        "  \n",
        "  classifier = Sequential()\n",
        "\n",
        "  ## Input Layer\n",
        "  classifier.add(Dense(No_Hidden_Neurons, activation=Hidden_Activation, \n",
        "                       kernel_initializer=Kernel_Initializer, input_dim=No_Features))\n",
        "  \n",
        "  ## Hidden layers\n",
        "  for i in range(No_Hidden_Layers):\n",
        "    classifier.add(Dense(No_Hidden_Neurons, activation=Hidden_Activation, \n",
        "                         kernel_initializer=Kernel_Initializer))\n",
        "    \n",
        "  ## Output Layer\n",
        "  classifier.add(Dense(No_OP_Neurons, activation=Output_Activation, \n",
        "                       kernel_initializer=Kernel_Initializer))\n",
        "  \n",
        "  classifier.compile(optimizer =Optimizer, loss=Loss, metrics = Metrics)\n",
        "\n",
        "  return classifier\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vld3KWRyOl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Train_NN(NN_classifier, train_data, feature_list=[], Batch_Size=50, Epochs=100):\n",
        "\n",
        "  train_data.dropna()\n",
        "  train_data = pd.DataFrame(np.nan_to_num(np.array(train_data)), columns = train_data.columns)\n",
        "  train_data['Label'] = pd.to_numeric(train_data['Label'], errors='coerce')\n",
        "  train_data = train_data.dropna(subset=['Label'])\n",
        "  \n",
        "  train_features = train_data[feature_list]    \n",
        "  train_labels = train_data[\"Label\"]\n",
        "  train_labels = train_labels.astype('int')\n",
        "\n",
        "  NN_classifier.fit(train_features,train_labels, batch_size=Batch_Size, epochs=Epochs)\n",
        "\n",
        "  eval_model=NN_classifier.evaluate(train_features, train_labels)\n",
        "  print(\"Loss: \", eval_model[0])\n",
        "  print(\"Accuracy of the model: \", eval_model[1])\n",
        "\n",
        "  return NN_classifier\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC_0KFa_5sUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Store trained model in a file to reuse in other codes without training again on same data\n",
        "\n",
        "def Store_Trained_NN(NN_obj, Filepath):\n",
        "  \n",
        "  with open(Filepath, \"wb\") as file:\n",
        "    pickle.dump(NN_obj, file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2qVC8-D5z3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Load stored trained model and returns random forest model object\n",
        "\n",
        "def Load_Trained_NN(Filepath):\n",
        "  \n",
        "  with open(Filepath, \"rb\") as file:\n",
        "    NN_obj = pickle.load(file)\n",
        "\n",
        "  return NN_obj"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb8oMI-x59MK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Evaluate_NN(test_data, NN_Model_FilePath, feature_list=[], threshold=0.5):\n",
        "  \n",
        "  test_data.dropna()\n",
        "  test_data = pd.DataFrame(np.nan_to_num(np.array(test_data)),  columns = test_data.columns)\n",
        "  test_data['Label'] = pd.to_numeric(test_data['Label'], errors='coerce')\n",
        "  test_data = test_data.dropna(subset=['Label'])\n",
        "\n",
        "  test_features = test_data[feature_list]\n",
        "  test_labels = test_data[\"Label\"]\n",
        "  test_labels = test_labels.astype('int')\n",
        "\n",
        "  NN_obj = Load_Trained_NN(NN_Model_FilePath) \n",
        "  predictions = NN_obj.predict(test_features)\n",
        "  predictions_list = [int(p[0]) for p in predictions]\n",
        "  \n",
        "  for i in range(len(predictions_list)):\n",
        "    if predictions_list[i] >= threshold:\n",
        "      predictions_list[i] = 1\n",
        "    else:\n",
        "      predictions_list[i] = 0\n",
        "  \n",
        "  errors = abs(predictions_list - test_labels)\n",
        "\n",
        "  # Calculate mean absolute error (MAE)\n",
        "  MAE = round(np.mean(errors), 2)\n",
        "  \n",
        "  ## Confusion Matrix and Classification Report\n",
        "  Confusion_Matrix = confusion_matrix(test_labels,predictions_list)\n",
        "  Report = classification_report(test_labels,predictions_list)\n",
        "  \n",
        "  return MAE, Confusion_Matrix, Report\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEll-MmJxY2l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ecc83983-462a-4a9c-b6c0-f690714c5263"
      },
      "source": [
        "## WORD2VEC EMBEDDINGS\n",
        "\n",
        "Column_List = [ \"Caption\"]\n",
        "Vector_Size = 300\n",
        "Embedding_Cols = [str(i) for i in range(Vector_Size)]\n",
        "Column_List.extend(Embedding_Cols)\n",
        "Column_List.append(\"Label\")\n",
        "\n",
        "Train_Embedding_FilePath = \"/content/TrainData_Word2Vec_Embeddings.csv\"\n",
        "Test_Embedding_FilePath = \"/content/TestData_Word2Vec_Embeddings.csv\"\n",
        "NN_Model_FilePath = \"/content/NN_Word2Vec_Train_Model.pkl\"\n",
        "\n",
        "train_data = pd.read_csv(Train_Embedding_FilePath, usecols=Column_List)\n",
        "test_data = pd.read_csv(Test_Embedding_FilePath, usecols=Column_List)\n",
        "\n",
        "## Training Phase\n",
        "NN_Classifier = Create_NN_Model()\n",
        "NN_obj = Train_NN(NN_Classifier, train_data, Embedding_Cols)\n",
        "Store_Trained_NN(NN_obj, NN_Model_FilePath)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "5824/5824 [==============================] - 1s 201us/step - loss: 0.4781 - acc: 0.7843\n",
            "Epoch 2/100\n",
            "5824/5824 [==============================] - 1s 161us/step - loss: 0.3546 - acc: 0.8372\n",
            "Epoch 3/100\n",
            "5824/5824 [==============================] - 1s 162us/step - loss: 0.3125 - acc: 0.8590\n",
            "Epoch 4/100\n",
            "5824/5824 [==============================] - 1s 161us/step - loss: 0.2784 - acc: 0.8750\n",
            "Epoch 5/100\n",
            "5824/5824 [==============================] - 1s 158us/step - loss: 0.2671 - acc: 0.8815\n",
            "Epoch 6/100\n",
            "5824/5824 [==============================] - 1s 157us/step - loss: 0.2325 - acc: 0.8987\n",
            "Epoch 7/100\n",
            "5824/5824 [==============================] - 1s 157us/step - loss: 0.2138 - acc: 0.9076\n",
            "Epoch 8/100\n",
            "5824/5824 [==============================] - 1s 152us/step - loss: 0.1980 - acc: 0.9181\n",
            "Epoch 9/100\n",
            "5824/5824 [==============================] - 1s 154us/step - loss: 0.1888 - acc: 0.9215\n",
            "Epoch 10/100\n",
            "5824/5824 [==============================] - 1s 147us/step - loss: 0.1624 - acc: 0.9351\n",
            "Epoch 11/100\n",
            "5824/5824 [==============================] - 1s 150us/step - loss: 0.1435 - acc: 0.9420\n",
            "Epoch 12/100\n",
            "5824/5824 [==============================] - 1s 156us/step - loss: 0.1283 - acc: 0.9487\n",
            "Epoch 13/100\n",
            "5824/5824 [==============================] - 1s 150us/step - loss: 0.1314 - acc: 0.9483\n",
            "Epoch 14/100\n",
            "5824/5824 [==============================] - 1s 147us/step - loss: 0.1122 - acc: 0.9511\n",
            "Epoch 15/100\n",
            "5824/5824 [==============================] - 1s 151us/step - loss: 0.1022 - acc: 0.9598\n",
            "Epoch 16/100\n",
            "5824/5824 [==============================] - 1s 153us/step - loss: 0.0840 - acc: 0.9677\n",
            "Epoch 17/100\n",
            "5824/5824 [==============================] - 1s 155us/step - loss: 0.0801 - acc: 0.9701\n",
            "Epoch 18/100\n",
            "5824/5824 [==============================] - 1s 149us/step - loss: 0.0716 - acc: 0.9718\n",
            "Epoch 19/100\n",
            "5824/5824 [==============================] - 1s 151us/step - loss: 0.0740 - acc: 0.9708\n",
            "Epoch 20/100\n",
            "5824/5824 [==============================] - 1s 153us/step - loss: 0.0641 - acc: 0.9749\n",
            "Epoch 21/100\n",
            "5824/5824 [==============================] - 1s 153us/step - loss: 0.0449 - acc: 0.9832\n",
            "Epoch 22/100\n",
            "5824/5824 [==============================] - 1s 151us/step - loss: 0.0588 - acc: 0.9770\n",
            "Epoch 23/100\n",
            "5824/5824 [==============================] - 1s 152us/step - loss: 0.0647 - acc: 0.9748\n",
            "Epoch 24/100\n",
            "5824/5824 [==============================] - 1s 156us/step - loss: 0.0616 - acc: 0.9761\n",
            "Epoch 25/100\n",
            "5824/5824 [==============================] - 1s 173us/step - loss: 0.0628 - acc: 0.9760\n",
            "Epoch 26/100\n",
            "5824/5824 [==============================] - 1s 168us/step - loss: 0.0473 - acc: 0.9827\n",
            "Epoch 27/100\n",
            "5824/5824 [==============================] - 1s 155us/step - loss: 0.0334 - acc: 0.9878\n",
            "Epoch 28/100\n",
            "5824/5824 [==============================] - 1s 154us/step - loss: 0.0466 - acc: 0.9820\n",
            "Epoch 29/100\n",
            "5824/5824 [==============================] - 1s 146us/step - loss: 0.0347 - acc: 0.9866\n",
            "Epoch 30/100\n",
            "5824/5824 [==============================] - 1s 152us/step - loss: 0.0379 - acc: 0.9839\n",
            "Epoch 31/100\n",
            "5824/5824 [==============================] - 1s 153us/step - loss: 0.0312 - acc: 0.9894\n",
            "Epoch 32/100\n",
            "5824/5824 [==============================] - 1s 154us/step - loss: 0.0363 - acc: 0.9871\n",
            "Epoch 33/100\n",
            "5824/5824 [==============================] - 1s 156us/step - loss: 0.0396 - acc: 0.9849\n",
            "Epoch 34/100\n",
            "5824/5824 [==============================] - 1s 149us/step - loss: 0.0534 - acc: 0.9792\n",
            "Epoch 35/100\n",
            "5824/5824 [==============================] - 1s 185us/step - loss: 0.0303 - acc: 0.9878\n",
            "Epoch 36/100\n",
            "5824/5824 [==============================] - 1s 165us/step - loss: 0.0255 - acc: 0.9909\n",
            "Epoch 37/100\n",
            "5824/5824 [==============================] - 1s 149us/step - loss: 0.0247 - acc: 0.9911\n",
            "Epoch 38/100\n",
            "5824/5824 [==============================] - 1s 153us/step - loss: 0.0109 - acc: 0.9966\n",
            "Epoch 39/100\n",
            "5824/5824 [==============================] - 1s 151us/step - loss: 0.0142 - acc: 0.9945\n",
            "Epoch 40/100\n",
            "5824/5824 [==============================] - 1s 150us/step - loss: 0.0202 - acc: 0.9931\n",
            "Epoch 41/100\n",
            "5824/5824 [==============================] - 1s 154us/step - loss: 0.0118 - acc: 0.9957\n",
            "Epoch 42/100\n",
            "5824/5824 [==============================] - 1s 153us/step - loss: 0.0137 - acc: 0.9945\n",
            "Epoch 43/100\n",
            "5824/5824 [==============================] - 1s 151us/step - loss: 0.0806 - acc: 0.9754\n",
            "Epoch 44/100\n",
            "5824/5824 [==============================] - 1s 152us/step - loss: 0.0385 - acc: 0.9873\n",
            "Epoch 45/100\n",
            "5824/5824 [==============================] - 1s 150us/step - loss: 0.0233 - acc: 0.9926\n",
            "Epoch 46/100\n",
            "5824/5824 [==============================] - 1s 153us/step - loss: 0.0153 - acc: 0.9945\n",
            "Epoch 47/100\n",
            "5824/5824 [==============================] - 1s 154us/step - loss: 0.0105 - acc: 0.9959\n",
            "Epoch 48/100\n",
            "5824/5824 [==============================] - 1s 154us/step - loss: 0.0130 - acc: 0.9943\n",
            "Epoch 49/100\n",
            "5824/5824 [==============================] - 1s 152us/step - loss: 0.0100 - acc: 0.9961\n",
            "Epoch 50/100\n",
            "5824/5824 [==============================] - 1s 150us/step - loss: 0.0189 - acc: 0.9931\n",
            "Epoch 51/100\n",
            "5824/5824 [==============================] - 1s 154us/step - loss: 0.0406 - acc: 0.9839\n",
            "Epoch 52/100\n",
            "5824/5824 [==============================] - 1s 152us/step - loss: 0.0284 - acc: 0.9894\n",
            "Epoch 53/100\n",
            "5824/5824 [==============================] - 1s 152us/step - loss: 0.0326 - acc: 0.9871\n",
            "Epoch 54/100\n",
            "5824/5824 [==============================] - 1s 153us/step - loss: 0.0341 - acc: 0.9871\n",
            "Epoch 55/100\n",
            "5824/5824 [==============================] - 1s 147us/step - loss: 0.0134 - acc: 0.9961\n",
            "Epoch 56/100\n",
            "5824/5824 [==============================] - 1s 153us/step - loss: 0.0234 - acc: 0.9923\n",
            "Epoch 57/100\n",
            "5824/5824 [==============================] - 1s 151us/step - loss: 0.0213 - acc: 0.9930\n",
            "Epoch 58/100\n",
            "5824/5824 [==============================] - 1s 156us/step - loss: 0.0070 - acc: 0.9981\n",
            "Epoch 59/100\n",
            "5824/5824 [==============================] - 1s 153us/step - loss: 0.0524 - acc: 0.9833\n",
            "Epoch 60/100\n",
            "5824/5824 [==============================] - 1s 146us/step - loss: 0.0187 - acc: 0.9940\n",
            "Epoch 61/100\n",
            "5824/5824 [==============================] - 1s 154us/step - loss: 0.0103 - acc: 0.9974\n",
            "Epoch 62/100\n",
            "5824/5824 [==============================] - 1s 152us/step - loss: 0.0080 - acc: 0.9976\n",
            "Epoch 63/100\n",
            "5824/5824 [==============================] - 1s 160us/step - loss: 0.0052 - acc: 0.9985\n",
            "Epoch 64/100\n",
            "5824/5824 [==============================] - 1s 153us/step - loss: 0.0105 - acc: 0.9957\n",
            "Epoch 65/100\n",
            "5824/5824 [==============================] - 1s 155us/step - loss: 0.0204 - acc: 0.9931\n",
            "Epoch 66/100\n",
            "5824/5824 [==============================] - 1s 154us/step - loss: 0.0111 - acc: 0.9962\n",
            "Epoch 67/100\n",
            "5824/5824 [==============================] - 1s 153us/step - loss: 0.0061 - acc: 0.9981\n",
            "Epoch 68/100\n",
            "5824/5824 [==============================] - 1s 152us/step - loss: 0.0032 - acc: 0.9985\n",
            "Epoch 69/100\n",
            "5824/5824 [==============================] - 1s 160us/step - loss: 0.0101 - acc: 0.9964\n",
            "Epoch 70/100\n",
            "5824/5824 [==============================] - 1s 184us/step - loss: 0.0073 - acc: 0.9966\n",
            "Epoch 71/100\n",
            "5824/5824 [==============================] - 1s 175us/step - loss: 0.0561 - acc: 0.9821\n",
            "Epoch 72/100\n",
            "5824/5824 [==============================] - 1s 173us/step - loss: 0.0162 - acc: 0.9948\n",
            "Epoch 73/100\n",
            "5824/5824 [==============================] - 1s 180us/step - loss: 0.0204 - acc: 0.9930\n",
            "Epoch 74/100\n",
            "5824/5824 [==============================] - 1s 204us/step - loss: 0.0273 - acc: 0.9902\n",
            "Epoch 75/100\n",
            "5824/5824 [==============================] - 1s 173us/step - loss: 0.0078 - acc: 0.9967\n",
            "Epoch 76/100\n",
            "5824/5824 [==============================] - 1s 176us/step - loss: 0.0103 - acc: 0.9966\n",
            "Epoch 77/100\n",
            "5824/5824 [==============================] - 1s 168us/step - loss: 0.0081 - acc: 0.9983\n",
            "Epoch 78/100\n",
            "5824/5824 [==============================] - 1s 165us/step - loss: 0.0020 - acc: 0.9993\n",
            "Epoch 79/100\n",
            "5824/5824 [==============================] - 1s 166us/step - loss: 0.0020 - acc: 0.9997\n",
            "Epoch 80/100\n",
            "5824/5824 [==============================] - 1s 160us/step - loss: 0.0021 - acc: 0.9993\n",
            "Epoch 81/100\n",
            "5824/5824 [==============================] - 1s 173us/step - loss: 0.0134 - acc: 0.9959\n",
            "Epoch 82/100\n",
            "5824/5824 [==============================] - 1s 179us/step - loss: 0.0175 - acc: 0.9942\n",
            "Epoch 83/100\n",
            "5824/5824 [==============================] - 1s 182us/step - loss: 0.0100 - acc: 0.9974\n",
            "Epoch 84/100\n",
            "5824/5824 [==============================] - 1s 193us/step - loss: 0.0451 - acc: 0.9833\n",
            "Epoch 85/100\n",
            "5824/5824 [==============================] - 1s 194us/step - loss: 0.0218 - acc: 0.9918\n",
            "Epoch 86/100\n",
            "5824/5824 [==============================] - 1s 196us/step - loss: 0.0090 - acc: 0.9973\n",
            "Epoch 87/100\n",
            "5824/5824 [==============================] - 1s 196us/step - loss: 0.0115 - acc: 0.9954\n",
            "Epoch 88/100\n",
            "5824/5824 [==============================] - 1s 186us/step - loss: 0.0124 - acc: 0.9943\n",
            "Epoch 89/100\n",
            "5824/5824 [==============================] - 1s 197us/step - loss: 0.0112 - acc: 0.9962\n",
            "Epoch 90/100\n",
            "5824/5824 [==============================] - 1s 188us/step - loss: 0.0268 - acc: 0.9907\n",
            "Epoch 91/100\n",
            "5824/5824 [==============================] - 1s 167us/step - loss: 0.0104 - acc: 0.9969\n",
            "Epoch 92/100\n",
            "5824/5824 [==============================] - 1s 161us/step - loss: 0.0287 - acc: 0.9900\n",
            "Epoch 93/100\n",
            "5824/5824 [==============================] - 1s 166us/step - loss: 0.0145 - acc: 0.9957\n",
            "Epoch 94/100\n",
            "5824/5824 [==============================] - 1s 181us/step - loss: 0.0104 - acc: 0.9962\n",
            "Epoch 95/100\n",
            "5824/5824 [==============================] - 1s 211us/step - loss: 0.0062 - acc: 0.9981\n",
            "Epoch 96/100\n",
            "5824/5824 [==============================] - 1s 210us/step - loss: 0.0011 - acc: 0.9998\n",
            "Epoch 97/100\n",
            "5824/5824 [==============================] - 1s 195us/step - loss: 4.5412e-04 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "5824/5824 [==============================] - 1s 192us/step - loss: 4.1688e-04 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "5824/5824 [==============================] - 1s 194us/step - loss: 1.8903e-04 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "5824/5824 [==============================] - 1s 204us/step - loss: 1.8534e-04 - acc: 1.0000\n",
            "5824/5824 [==============================] - 0s 50us/step\n",
            "Loss:  0.00010309237324989496\n",
            "Accuracy of the model:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkCoBdcZ7ktW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "b71a45ad-ec0f-491d-f5ea-ec2c5924ee2f"
      },
      "source": [
        "## Testing Phase\n",
        "MAE, Confusion_Matrix, Report = Evaluate_NN(test_data, NN_Model_FilePath, Embedding_Cols, 0.5)\n",
        "\n",
        "print(\"============ FOR WORD2VEC EMBEDDINGS ============\")\n",
        "\n",
        "print(\"MEAN ABSOLUTE ERROR: \", MAE)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"============ CONFUSION MATRIX ===============\")\n",
        "print(Confusion_Matrix)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"============ CLASSIFICATION REPORT ==============\")\n",
        "print(Report)\n",
        "\n",
        "tn, fp, fn, tp = Confusion_Matrix.ravel()\n",
        "Accuracy = (tn+tp)/(tn + fp + fn + tp)\n",
        "\n",
        "print(\"Accuracy: \", Accuracy*100)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============ FOR WORD2VEC EMBEDDINGS ============\n",
            "MEAN ABSOLUTE ERROR:  0.26\n",
            "\n",
            "\n",
            "============ CONFUSION MATRIX ===============\n",
            "[[728   5]\n",
            " [371 352]]\n",
            "\n",
            "\n",
            "============ CLASSIFICATION REPORT ==============\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.99      0.79       733\n",
            "           1       0.99      0.49      0.65       723\n",
            "\n",
            "    accuracy                           0.74      1456\n",
            "   macro avg       0.82      0.74      0.72      1456\n",
            "weighted avg       0.82      0.74      0.72      1456\n",
            "\n",
            "Accuracy:  74.17582417582418\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_q37EKAxm15",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3f612f42-3150-46e0-c8b0-e7daae873ad1"
      },
      "source": [
        "## GLOVE EMBEDDINGS\n",
        "\n",
        "Column_List = [ \"Caption_Tokens\"]\n",
        "Vector_Size = 300\n",
        "Embedding_Cols = [str(i) for i in range(Vector_Size)]\n",
        "Column_List.extend(Embedding_Cols)\n",
        "Column_List.append(\"Label\")\n",
        "\n",
        "Train_Embedding_FilePath = \"/content/TrainData_Glove_Embeddings.csv\"\n",
        "Test_Embedding_FilePath = \"/content/TestData_Glove_Embeddings.csv\"\n",
        "NN_Model_FilePath = \"/content/NN_Glove_Train_Model.pkl\"\n",
        "\n",
        "train_data = pd.read_csv(Train_Embedding_FilePath, usecols=Column_List)\n",
        "test_data = pd.read_csv(Test_Embedding_FilePath, usecols=Column_List)\n",
        "\n",
        "## Training Phase\n",
        "NN_Classifier = Create_NN_Model()\n",
        "NN_obj = Train_NN(NN_Classifier, train_data, Embedding_Cols)\n",
        "Store_Trained_NN(NN_obj, NN_Model_FilePath)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "5824/5824 [==============================] - 1s 208us/step - loss: 0.5503 - acc: 0.7141\n",
            "Epoch 2/100\n",
            "5824/5824 [==============================] - 1s 154us/step - loss: 0.4807 - acc: 0.7543\n",
            "Epoch 3/100\n",
            "5824/5824 [==============================] - 1s 162us/step - loss: 0.4649 - acc: 0.7632\n",
            "Epoch 4/100\n",
            "5824/5824 [==============================] - 1s 161us/step - loss: 0.4491 - acc: 0.7720\n",
            "Epoch 5/100\n",
            "5824/5824 [==============================] - 1s 156us/step - loss: 0.4426 - acc: 0.7766\n",
            "Epoch 6/100\n",
            "5824/5824 [==============================] - 1s 161us/step - loss: 0.4350 - acc: 0.7806\n",
            "Epoch 7/100\n",
            "5824/5824 [==============================] - 1s 158us/step - loss: 0.4266 - acc: 0.7912\n",
            "Epoch 8/100\n",
            "5824/5824 [==============================] - 1s 152us/step - loss: 0.4212 - acc: 0.7878\n",
            "Epoch 9/100\n",
            "5824/5824 [==============================] - 1s 156us/step - loss: 0.4160 - acc: 0.7919\n",
            "Epoch 10/100\n",
            "5824/5824 [==============================] - 1s 151us/step - loss: 0.4140 - acc: 0.7902\n",
            "Epoch 11/100\n",
            "5824/5824 [==============================] - 1s 164us/step - loss: 0.4072 - acc: 0.7979\n",
            "Epoch 12/100\n",
            "5824/5824 [==============================] - 1s 160us/step - loss: 0.4088 - acc: 0.7986\n",
            "Epoch 13/100\n",
            "5824/5824 [==============================] - 1s 155us/step - loss: 0.3996 - acc: 0.7995\n",
            "Epoch 14/100\n",
            "5824/5824 [==============================] - 1s 155us/step - loss: 0.3931 - acc: 0.8053\n",
            "Epoch 15/100\n",
            "5824/5824 [==============================] - 1s 157us/step - loss: 0.3921 - acc: 0.8001\n",
            "Epoch 16/100\n",
            "5824/5824 [==============================] - 1s 146us/step - loss: 0.3880 - acc: 0.8044\n",
            "Epoch 17/100\n",
            "5824/5824 [==============================] - 1s 154us/step - loss: 0.3788 - acc: 0.8098\n",
            "Epoch 18/100\n",
            "5824/5824 [==============================] - 1s 148us/step - loss: 0.3794 - acc: 0.8087\n",
            "Epoch 19/100\n",
            "5824/5824 [==============================] - 1s 152us/step - loss: 0.3777 - acc: 0.8163\n",
            "Epoch 20/100\n",
            "5824/5824 [==============================] - 1s 169us/step - loss: 0.3694 - acc: 0.8218\n",
            "Epoch 21/100\n",
            "5824/5824 [==============================] - 1s 208us/step - loss: 0.3677 - acc: 0.8195\n",
            "Epoch 22/100\n",
            "5824/5824 [==============================] - 1s 174us/step - loss: 0.3648 - acc: 0.8237\n",
            "Epoch 23/100\n",
            "5824/5824 [==============================] - 1s 151us/step - loss: 0.3619 - acc: 0.8231\n",
            "Epoch 24/100\n",
            "5824/5824 [==============================] - 1s 151us/step - loss: 0.3583 - acc: 0.8274\n",
            "Epoch 25/100\n",
            "5824/5824 [==============================] - 1s 155us/step - loss: 0.3588 - acc: 0.8243\n",
            "Epoch 26/100\n",
            "5824/5824 [==============================] - 1s 153us/step - loss: 0.3496 - acc: 0.8269\n",
            "Epoch 27/100\n",
            "5824/5824 [==============================] - 1s 145us/step - loss: 0.3410 - acc: 0.8341\n",
            "Epoch 28/100\n",
            "5824/5824 [==============================] - 1s 149us/step - loss: 0.3436 - acc: 0.8341\n",
            "Epoch 29/100\n",
            "5824/5824 [==============================] - 1s 147us/step - loss: 0.3366 - acc: 0.8364\n",
            "Epoch 30/100\n",
            "5824/5824 [==============================] - 1s 145us/step - loss: 0.3304 - acc: 0.8456\n",
            "Epoch 31/100\n",
            "5824/5824 [==============================] - 1s 146us/step - loss: 0.3297 - acc: 0.8365\n",
            "Epoch 32/100\n",
            "5824/5824 [==============================] - 1s 144us/step - loss: 0.3260 - acc: 0.8388\n",
            "Epoch 33/100\n",
            "5824/5824 [==============================] - 1s 149us/step - loss: 0.3162 - acc: 0.8474\n",
            "Epoch 34/100\n",
            "5824/5824 [==============================] - 1s 147us/step - loss: 0.3209 - acc: 0.8450\n",
            "Epoch 35/100\n",
            "5824/5824 [==============================] - 1s 159us/step - loss: 0.3100 - acc: 0.8529\n",
            "Epoch 36/100\n",
            "5824/5824 [==============================] - 1s 152us/step - loss: 0.3040 - acc: 0.8534\n",
            "Epoch 37/100\n",
            "5824/5824 [==============================] - 1s 153us/step - loss: 0.3080 - acc: 0.8515\n",
            "Epoch 38/100\n",
            "5824/5824 [==============================] - 1s 154us/step - loss: 0.3030 - acc: 0.8492\n",
            "Epoch 39/100\n",
            "5824/5824 [==============================] - 1s 148us/step - loss: 0.2989 - acc: 0.8583\n",
            "Epoch 40/100\n",
            "5824/5824 [==============================] - 1s 147us/step - loss: 0.2930 - acc: 0.8580\n",
            "Epoch 41/100\n",
            "5824/5824 [==============================] - 1s 142us/step - loss: 0.2870 - acc: 0.8642\n",
            "Epoch 42/100\n",
            "5824/5824 [==============================] - 1s 145us/step - loss: 0.2853 - acc: 0.8618\n",
            "Epoch 43/100\n",
            "5824/5824 [==============================] - 1s 143us/step - loss: 0.2750 - acc: 0.8697\n",
            "Epoch 44/100\n",
            "5824/5824 [==============================] - 1s 143us/step - loss: 0.2776 - acc: 0.8654\n",
            "Epoch 45/100\n",
            "5824/5824 [==============================] - 1s 147us/step - loss: 0.2710 - acc: 0.8736\n",
            "Epoch 46/100\n",
            "5824/5824 [==============================] - 1s 150us/step - loss: 0.2638 - acc: 0.8788\n",
            "Epoch 47/100\n",
            "5824/5824 [==============================] - 1s 149us/step - loss: 0.2691 - acc: 0.8750\n",
            "Epoch 48/100\n",
            "5824/5824 [==============================] - 1s 147us/step - loss: 0.2648 - acc: 0.8726\n",
            "Epoch 49/100\n",
            "5824/5824 [==============================] - 1s 149us/step - loss: 0.2488 - acc: 0.8817\n",
            "Epoch 50/100\n",
            "5824/5824 [==============================] - 1s 147us/step - loss: 0.2539 - acc: 0.8802\n",
            "Epoch 51/100\n",
            "5824/5824 [==============================] - 1s 147us/step - loss: 0.2361 - acc: 0.8894\n",
            "Epoch 52/100\n",
            "5824/5824 [==============================] - 1s 148us/step - loss: 0.2351 - acc: 0.8910\n",
            "Epoch 53/100\n",
            "5824/5824 [==============================] - 1s 150us/step - loss: 0.2371 - acc: 0.8882\n",
            "Epoch 54/100\n",
            "5824/5824 [==============================] - 1s 147us/step - loss: 0.2365 - acc: 0.8893\n",
            "Epoch 55/100\n",
            "5824/5824 [==============================] - 1s 144us/step - loss: 0.2257 - acc: 0.8999\n",
            "Epoch 56/100\n",
            "5824/5824 [==============================] - 1s 147us/step - loss: 0.2170 - acc: 0.8980\n",
            "Epoch 57/100\n",
            "5824/5824 [==============================] - 1s 144us/step - loss: 0.2077 - acc: 0.9059\n",
            "Epoch 58/100\n",
            "5824/5824 [==============================] - 1s 153us/step - loss: 0.2105 - acc: 0.9045\n",
            "Epoch 59/100\n",
            "5824/5824 [==============================] - 1s 147us/step - loss: 0.2105 - acc: 0.9028\n",
            "Epoch 60/100\n",
            "5824/5824 [==============================] - 1s 146us/step - loss: 0.2277 - acc: 0.8984\n",
            "Epoch 61/100\n",
            "5824/5824 [==============================] - 1s 148us/step - loss: 0.1988 - acc: 0.9154\n",
            "Epoch 62/100\n",
            "5824/5824 [==============================] - 1s 151us/step - loss: 0.2030 - acc: 0.9090\n",
            "Epoch 63/100\n",
            "5824/5824 [==============================] - 1s 149us/step - loss: 0.1893 - acc: 0.9131\n",
            "Epoch 64/100\n",
            "5824/5824 [==============================] - 1s 148us/step - loss: 0.1939 - acc: 0.9143\n",
            "Epoch 65/100\n",
            "5824/5824 [==============================] - 1s 147us/step - loss: 0.1763 - acc: 0.9207\n",
            "Epoch 66/100\n",
            "5824/5824 [==============================] - 1s 147us/step - loss: 0.1768 - acc: 0.9239\n",
            "Epoch 67/100\n",
            "5824/5824 [==============================] - 1s 143us/step - loss: 0.1711 - acc: 0.9231\n",
            "Epoch 68/100\n",
            "5824/5824 [==============================] - 1s 145us/step - loss: 0.1643 - acc: 0.9257\n",
            "Epoch 69/100\n",
            "5824/5824 [==============================] - 1s 144us/step - loss: 0.1711 - acc: 0.9231\n",
            "Epoch 70/100\n",
            "5824/5824 [==============================] - 1s 147us/step - loss: 0.1817 - acc: 0.9222\n",
            "Epoch 71/100\n",
            "5824/5824 [==============================] - 1s 151us/step - loss: 0.1768 - acc: 0.9188\n",
            "Epoch 72/100\n",
            "5824/5824 [==============================] - 1s 150us/step - loss: 0.1696 - acc: 0.9251\n",
            "Epoch 73/100\n",
            "5824/5824 [==============================] - 1s 149us/step - loss: 0.1551 - acc: 0.9298\n",
            "Epoch 74/100\n",
            "5824/5824 [==============================] - 1s 145us/step - loss: 0.1456 - acc: 0.9346\n",
            "Epoch 75/100\n",
            "5824/5824 [==============================] - 1s 168us/step - loss: 0.1458 - acc: 0.9348\n",
            "Epoch 76/100\n",
            "5824/5824 [==============================] - 1s 182us/step - loss: 0.1360 - acc: 0.9437\n",
            "Epoch 77/100\n",
            "5824/5824 [==============================] - 1s 145us/step - loss: 0.1454 - acc: 0.9380\n",
            "Epoch 78/100\n",
            "5824/5824 [==============================] - 1s 146us/step - loss: 0.1642 - acc: 0.9296\n",
            "Epoch 79/100\n",
            "5824/5824 [==============================] - 1s 149us/step - loss: 0.1368 - acc: 0.9418\n",
            "Epoch 80/100\n",
            "5824/5824 [==============================] - 1s 146us/step - loss: 0.1294 - acc: 0.9440\n",
            "Epoch 81/100\n",
            "5824/5824 [==============================] - 1s 147us/step - loss: 0.1469 - acc: 0.9344\n",
            "Epoch 82/100\n",
            "5824/5824 [==============================] - 1s 150us/step - loss: 0.1455 - acc: 0.9365\n",
            "Epoch 83/100\n",
            "5824/5824 [==============================] - 1s 149us/step - loss: 0.1429 - acc: 0.9366\n",
            "Epoch 84/100\n",
            "5824/5824 [==============================] - 1s 152us/step - loss: 0.1151 - acc: 0.9511\n",
            "Epoch 85/100\n",
            "5824/5824 [==============================] - 1s 147us/step - loss: 0.1134 - acc: 0.9516\n",
            "Epoch 86/100\n",
            "5824/5824 [==============================] - 1s 148us/step - loss: 0.1239 - acc: 0.9468\n",
            "Epoch 87/100\n",
            "5824/5824 [==============================] - 1s 186us/step - loss: 0.1158 - acc: 0.9514\n",
            "Epoch 88/100\n",
            "5824/5824 [==============================] - 1s 154us/step - loss: 0.1148 - acc: 0.9516\n",
            "Epoch 89/100\n",
            "5824/5824 [==============================] - 1s 201us/step - loss: 0.1303 - acc: 0.9432\n",
            "Epoch 90/100\n",
            "5824/5824 [==============================] - 1s 192us/step - loss: 0.1223 - acc: 0.9504\n",
            "Epoch 91/100\n",
            "5824/5824 [==============================] - 1s 150us/step - loss: 0.1192 - acc: 0.9518\n",
            "Epoch 92/100\n",
            "5824/5824 [==============================] - 1s 146us/step - loss: 0.1780 - acc: 0.9370\n",
            "Epoch 93/100\n",
            "5824/5824 [==============================] - 1s 150us/step - loss: 0.1091 - acc: 0.9518\n",
            "Epoch 94/100\n",
            "5824/5824 [==============================] - 1s 143us/step - loss: 0.0933 - acc: 0.9610\n",
            "Epoch 95/100\n",
            "5824/5824 [==============================] - 1s 147us/step - loss: 0.1082 - acc: 0.9528\n",
            "Epoch 96/100\n",
            "5824/5824 [==============================] - 1s 145us/step - loss: 0.1047 - acc: 0.9567\n",
            "Epoch 97/100\n",
            "5824/5824 [==============================] - 1s 143us/step - loss: 0.1214 - acc: 0.9478\n",
            "Epoch 98/100\n",
            "5824/5824 [==============================] - 1s 146us/step - loss: 0.1125 - acc: 0.9557\n",
            "Epoch 99/100\n",
            "5824/5824 [==============================] - 1s 145us/step - loss: 0.0972 - acc: 0.9600\n",
            "Epoch 100/100\n",
            "5824/5824 [==============================] - 1s 148us/step - loss: 0.1031 - acc: 0.9598\n",
            "5824/5824 [==============================] - 0s 56us/step\n",
            "Loss:  0.0716506925431991\n",
            "Accuracy of the model:  0.9716689560439561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvNiLuWJ7rWi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "628f7915-9376-42ae-c2e4-e63633a3a8aa"
      },
      "source": [
        "## Testing Phase\n",
        "MAE, Confusion_Matrix, Report = Evaluate_NN(test_data, NN_Model_FilePath, Embedding_Cols, 0.5)\n",
        "\n",
        "print(\"============ FOR GLOVE EMBEDDINGS ============\")\n",
        "\n",
        "print(\"MEAN ABSOLUTE ERROR: \", MAE)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"============ CONFUSION MATRIX ===============\")\n",
        "print(Confusion_Matrix)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"============ CLASSIFICATION REPORT ==============\")\n",
        "print(Report)\n",
        "\n",
        "tn, fp, fn, tp = Confusion_Matrix.ravel()\n",
        "Accuracy = (tn+tp)/(tn + fp + fn + tp)\n",
        "\n",
        "print(\"Accuracy: \", Accuracy*100)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============ FOR GLOVE EMBEDDINGS ============\n",
            "MEAN ABSOLUTE ERROR:  0.4\n",
            "\n",
            "\n",
            "============ CONFUSION MATRIX ===============\n",
            "[[713   4]\n",
            " [578 161]]\n",
            "\n",
            "\n",
            "============ CLASSIFICATION REPORT ==============\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.99      0.71       717\n",
            "           1       0.98      0.22      0.36       739\n",
            "\n",
            "    accuracy                           0.60      1456\n",
            "   macro avg       0.76      0.61      0.53      1456\n",
            "weighted avg       0.77      0.60      0.53      1456\n",
            "\n",
            "Accuracy:  60.027472527472526\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}